# Default configuration for kinesis-lzo-s3-sink

sink {

  # The following are used to authenticate for the Amazon Kinesis sink.
  #
  # If both are set to 'default', the default provider chain is used
  # (see http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/auth/DefaultAWSCredentialsProviderChain.html)
  #
  # If both are set to 'iam', use AWS IAM Roles to provision credentials.
  #
  # If both are set to 'env', use environment variables AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY
  aws {
    access-key: ""
    secret-key: ""
  }

  kinesis {
    in {
      # Kinesis input stream name
      stream-name: "SnowplowEnriched"

      # LATEST: most recent data.
      # TRIM_HORIZON: oldest available data.
      # Note: This only affects the first run of this application
      # on a stream.
      initial-position: "TRIM_HORIZON"
    }

    out {
      # Stream for events for which the storage process fails
      stream-name: "SnowplowRedshiftBad"
    }

    region: "ap-southeast-2"

    # "app-name" is used for a DynamoDB table to maintain stream state.
    # You can set it automatically using: "SnowplowLzoS3Sink-$\\{sink.kinesis.in.stream-name\\}"
    app-name: "KinesisRedshiftApp"
  }

  redshift {
    # Redshift endpoint
    url: "jdbc:postgresql://snowplow.digdeep.digdeepdigital.com.au:5439/snowplow"
    username: <DB User>
    password: <DB password>

    # The events table in each schema
    table: events

    # Default schema (typically atomic)
    defaultSchema: test

    # Advanced - Use this to customize where events will go depending on app_id. Otherwise, they'll all got into the default DB schema
#    appIdToSchema: "digdeep:test"

    # hostname or IP or the host which will produce event files - Redshift should be allowed to access it via SSH (for COPY SSH)
    sshEndpoint:
    sshUsername: ubuntu

    # Where dynamic SSH manifests are stored, uses the same credentials as Kinesis Client Library
    sshS3Folder: "digdeep-data-dump/redshift_realtime"

    # Where jsonpath files are stored, needs to be in two formats
    jsonPaths: "s3://digdeep-snowplow-hosted-assets" # where
    jsonpaths: "http://digdeep-snowplow-hosted-assets.s3-website-ap-southeast-2.amazonaws.com"

    # access and secret key for Redshift to be able to read the manifest files
    s3AccessKey:
    s3SecretKey:

    # Write time to collection time ratio, as string e.g. "1/10" - Redshift will spend only 10% of time on writes, regardless of throughput
    flushRatio: "1/10"

    # Number of events - how much the algorithm will wait until flushing accumulated events (either this or maxCollectionTime)
    batchSize: 100000

    # in ms, how long the algorithm will wait until flushing accumulated events for the first time, until it starts adapting
    defaultCollectionTime: 100000

    # in ms, maximum of how long the algorithm will wait until flushing accumulated events (either this or batchSize)
    maxCollectionTime: 300000

    # Where the logs are stored - all logs are automatically shipped to S3 to this location
#    s3LoggingBucket: "digdeep-data-dump"
#    s3LoggingPath: "redshift_realtime/logs" # No slash at the end

    # Where to publish the shredding metrics. Uses the same credentials as Kinesis Client Library
    cloudWatchNamespace: RealtimeShred

    # The port for the config interface which allows modification of the redshift parameters on the fly
#    configPort: 8080

    # Throttling configuration (requests/per second), per appId. Missing appIds are not throttled.
#   appIdToThrottle: "DOSAttack:100"

    # Whether to deduplicate the records (optional, boolean)
    deduplicate: true
  }

  # Events are accumulated in a buffer before being sent to S3.
  # The buffer is emptied whenever:
  # - the combined size of the stored records exceeds byte-limit or
  # - the number of stored records exceeds record-limit or
  # - the time in milliseconds since it was last emptied exceeds time-limit
  buffer {
    byte-limit: 4000000
    record-limit: 5000
    time-limit: 300000
  }

  # The S3 location of the jsonpaths, similar to what is typically given to the storage loader
  jsonpaths: "http://digdeep-snowplow-hosted-assets.s3-website-ap-southeast-2.amazonaws.com"

  # Iglu config, verbatim JSON, add your repository below
  iglu_config: """{
                   "schema": "iglu:com.snowplowanalytics.iglu/resolver-config/jsonschema/1-0-0",
                   "data": {
                     "cacheSize": 500,
                     "repositories": [
                       {
                         "name": "Iglu Central",
                         "priority": 0,
                         "vendorPrefixes": [
                           "com.snowplowanalytics"
                         ],
                         "connection": {
                           "http": {
                             "uri": "http://iglucentral.com"
                           }
                         }
                       }
                     ]
                   }
               }"""

}
