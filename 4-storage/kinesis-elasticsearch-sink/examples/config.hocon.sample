# Copyright (c) 2014-2016 Snowplow Analytics Ltd. All rights reserved.
#
# This program is licensed to you under the Apache License Version 2.0, and
# you may not use this file except in compliance with the Apache License
# Version 2.0.  You may obtain a copy of the Apache License Version 2.0 at
# http://www.apache.org/licenses/LICENSE-2.0.
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the Apache License Version 2.0 is distributed on an "AS
# IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
# implied.  See the Apache License Version 2.0 for the specific language
# governing permissions and limitations there under.

# This file (application.conf.example) contains a template with
# configuration options for the Kinesis Elasticsearch Sink.

sink {

  # Sources currently supported are:
  # 'kinesis' for reading records from a Kinesis stream
  # 'stdin' for reading unencoded tab-separated events from stdin
  # If set to "stdin", JSON documents will not be sent to Elasticsearch
  # but will be written to stdout.
  source = {{sinkElasticsearchInputType}}

  # Where to write good and bad records
  sink {
    # Sinks currently supported are:
    # 'elasticsearch' for writing good records to Elasticsearch
    # 'stdout' for writing good records to stdout
    "good": {{sinkElasticsearchGoodOutputDestination}}

    # Sinks currently supported are:
    # 'kinesis' for writing bad records to Kinesis
    # 'stderr' for writing bad records to stderr
    # 'none' for ignoring bad records
    "bad": {{sinkElasticsearchBadOutputDestination}}
  }

  # "good" for a stream of successfully enriched events
  # "bad" for a stream of bad events
  stream-type: "{{sinkKinesisInStreamType}}"

  # The following are used to authenticate for the Amazon Kinesis sink.
  #
  # If both are set to 'default', the default provider chain is used
  # (see http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/auth/DefaultAWSCredentialsProviderChain.html)
  #
  # If both are set to 'iam', use AWS IAM Roles to provision credentials.
  #
  # If both are set to 'env', use environment variables AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY
  aws {
    access-key: "iam"
    secret-key: "iam"
  }

  kinesis {

    in {
      stream-name: "{{sinkKinesisInStreamName}}" # Kinesis stream name

      # LATEST: most recent data.
      # TRIM_HORIZON: oldest available data.
      # Note: This only affects the first run of this application
      # on a stream.
      initial-position: "TRIM_HORIZON"

      # Maximum number of records to get from Kinesis per call to GetRecords
      maxRecords: 10000
    }

    out {
      # Stream for enriched events which are rejected by Elasticsearch
      stream-name: "{{sinkKinesisOutStreamName}}"
      shards: 1
    }

    region: "{{sinkElasticsearchRegion}}"

    # "app-name" is used for a DynamoDB table to maintain stream state.
    # You can set it automatically using: "SnowplowElasticsearchSink-$\\{connector.kinesis.in.stream-name\\}"
    app-name: "{{sinkKinesisAppName}}"
  }

  elasticsearch {

    # Events are indexed using an Elasticsearch Client
    # - type: http or transport (will default to transport)
    # - endpoint: the cluster endpoint
    # - port: the port the cluster can be accessed on
    #   - for http this is usually 9200
    #   - for transport this is usually 9300
    # - max-timeout: the maximum attempt time before a client restart
    client {
      type: "{{sinkElasticseachClient}}"
      endpoint: "{{sinkElasticsearchEndpoint}}"
      port: {{sinkElasticsearchTransportPort}}
      max-timeout: "{{sinkElasticsearchMaxTimeout}}"
    }

    cluster {
      name: "{{sinkElasticsearchClusterName}}"
      index: "{{sinkElasticsearchIndex}}"
      type: "{{sinkElasticsearchType}}"
    }
  }

  # Events are accumulated in a buffer before being sent to Elasticsearch.
  # The buffer is emptied whenever:
  # - the combined size of the stored records exceeds byte-limit or
  # - the number of stored records exceeds record-limit or
  # - the time in milliseconds since it was last emptied exceeds time-limit
  buffer {
    byte-limit: {{sinkElasticsearchBufferByteThreshold}}
    record-limit: {{sinkElasticsearchBufferRecordThreshold}}
    time-limit: {{sinkElasticsearchBufferTimeThreshold}}
  }

  # Optional section for tracking endpoints
  monitoring {
    snowplow {
      collector-uri: "{{collectorUri}}"
      collector-port: 80
      app-id: "{{sinkKinesisAppName}}"
      method: "GET"
    }
  }
}
