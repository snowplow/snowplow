# Copyright (c) 2013-2016 Snowplow Analytics Ltd. All rights reserved.
#
# This program is licensed to you under the Apache License Version 2.0, and
# you may not use this file except in compliance with the Apache License
# Version 2.0.  You may obtain a copy of the Apache License Version 2.0 at
# http://www.apache.org/licenses/LICENSE-2.0.
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the Apache License Version 2.0 is distributed on an "AS
# IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
# implied.  See the Apache License Version 2.0 for the specific language
# governing permissions and limitations there under.

# This file (application.conf.example) contains a template with
# configuration options for Stream Enrich.

enrich {
  # Sources currently supported are:
  # 'kinesis' for reading Thrift-serialized records from a Kinesis stream
  # 'kafka' for reading Thrift-serialized records from a Kafka topic
  # 'stdin' for reading Base64-encoded Thrift-serialized records from stdin
  source = "kinesis"

  # Sinks currently supported are:
  # 'kinesis' for writing enriched events to one Kinesis stream and invalid events to another.
  # 'kafka' for writing enriched events to one Kafka topic and invalid events to another.
  # 'stdouterr' for writing enriched events to stdout and invalid events to stderr.
  #    Using "sbt assembly" and "java -jar" is recommended to disable sbt
  #    logging.
  sink = "kinesis"

  # AWS credentials
  #
  # If both are set to 'default', use the default AWS credentials provider chain.
  #
  # If both are set to 'iam', use AWS IAM Roles to provision credentials.
  #
  # If both are set to 'env', use environment variables AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY
  aws {
    access-key = "iam"
    secret-key = "iam"
  }

  # Kafka configuration
  kafka {
    topic {
      in = "snowplow-collector-good.thrift"
      enriched = "snowplow-enrich-good.tsv"
      bad = "snowplow-enrich-bad"
      # this is how optional environment variables work
      in = ${?ENRICH_KAFKA_TOPIC_IN}
      enriched = ${?ENRICH_KAFKA_TOPIC_ENRICHED}
      bad = ${?ENRICH_KAFKA_TOPIC_BAD}
    }

    consumer {
      bootstrap.servers = "localhost:9092"
      bootstrap.servers = ${?ENRICH_KAFKA_PRODUCER_BOOTSTRAP_SERVERS}
      # you cannot specify or override group.id as it is defined using source.streams.app-name

      # store any additional Kafka native configuration here
      # example:
      # security.protocol = SSL
      # timeout.ms = 60000
    }

    producer {
      bootstrap.servers = "localhost:9092"
      bootstrap.servers = ${?ENRICH_KAFKA_PRODUCER_BOOTSTRAP_SERVERS}
      # store any additional Kafka native configuration here
      # example:
      # security.protocol = SSL
      # timeout.ms = 60000
    }
  }

  streams {
    in {
      raw = ${ENRICH_STREAM_IN_RAW}

      # Maximum number of records to get from Kinesis per call to GetRecords
      maxRecords = 10000

      # After enrichment, are accumulated in a buffer before being sent to Kinesis.
      # The buffer is emptied whenever:
      # - the number of stored records reaches record-limit or
      # - the combined size of the stored records reaches byte-limit or
      # - the time in milliseconds since it was last emptied exceeds time-limit when
      #   a new event enters the buffer
      buffer {
        byte-limit = 4000000
        record-limit = 500
        time-limit = 60000
        byte-limit = ${?ENRICH_STREAMS_IN_BUFFER_BYTE_LIMIT}
        record-limit = ${?ENRICH_STREAMS_IN_BUFFER_RECORD_LIMIT}
        time-limit = ${?ENRICH_STREAMS_IN_BUFFER_TIME_LIMIT}
      }
    }

    out {
      enriched = ${ENRICH_STREAMS_OUT_ENRICHED}
      bad = ${ENRICH_STREAMS_OUT_BAD}

      # Minimum and maximum backoff periods
      # - Units: Milliseconds
      backoffPolicy = {
        minBackoff = 3000
        maxBackoff = 60000
        minBackoff = ${?ENRICH_STREAMS_OUT_BACKOFFPOLICY_MIN}
        maxBackoff = ${?ENRICH_STREAMS_OUT_BACKOFFPOLICY_MAX}
      }
    }

    # "app-name" is used for a DynamoDB table to maintain stream state.
    # "app-name" is used as the Kafka consumer group ID.
    # You can set it automatically using: "SnowplowKinesisEnrich-$\\{enrich.streams.in.raw\\}
    app-name = "snowplow-enrich"
    app-name = ${?ENRICH_STREAMS_APP_NAME}

    # LATEST: most recent data.
    # TRIM_HORIZON: oldest available data.
    # Note: This only effects the first run of this application
    # on a stream.
    initial-position = "TRIM_HORIZON"

    region = ${ENRICH_STREAMS_REGION}
  }

  # Optional section for tracking endpoints
  monitoring {
    snowplow {
      collector-uri = ${MONITORING_SNOWPLOW_COLLECTOR_UI}
      collector-port = 80
      app-name = "snowplow-enrich"
      app-id = ${?ENRICH_STREAMS_APP_NAME}
      method = "GET"
    }
  }
}
